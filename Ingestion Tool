# this is old code using delimiter
import pandas as pd
import os
import json
import requests
import time
from pymongo import MongoClient
from datetime import datetime

# Connect to MongoDB
client = MongoClient('mongodb://localhost:27017/')
db = client['Database_Leaks']
collection = db['2,844 Separate Data Breaches3']

# Define the base schema
base_obj = {
    "id": "",
    "username": "",
    "email": "",
    "password": "",
    "password_hash": "",
    "name": "",
    "dob": "",
    "phone": "",
    "address": "",
    "source": "",
    "breach_date": "",
    "domainName": "",
    "others": ""
}

chunk_size = 5000
max_file_size = 200 * 1024 * 1024  # 200 MB in bytes

# Function to process a single CSV file
def process_csv(file_path, delimiter, pattern, chunk_size):
    # Implement your CSV processing logic here
    pass

# Function to process a single JSON file
def process_json(file_path, pattern, chunk_size):
    # Implement your JSON processing logic here
    pass

# Function to process a single SQL file
def process_sql(file_path, pattern):
    # Implement your SQL processing logic here
    pass

# Function to process a single TXT file
def process_txt_file(file_path, delimiter, pattern, chunk_size, breach_date, source_name):
    documents = []  # List to store documents
    skipped = 0

    # Create a list to store skipped lines
    skipped_lines = []

    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
        print('Processing', file_path)
        cnt = 0
        chunk_count = 0  # Initialize chunk count
        for line in file:
            cnt = cnt + 1
            line = line.strip()
            if delimiter in line:  # Check if the specified delimiter is in the line
                email, password = line.split(delimiter, 1)  # Split based on the specified delimiter
                domain_name = email.split('@')[-1]  # Extract domain name from email

                # Create a new record based on the base schema
                record = base_obj.copy()
                record['email'] = email
                if len(password) >= 16:
                    record['password_hash'] = password
                else:
                    record['password'] = password
                record['source'] = source_name
                record['domainName'] = domain_name
                record['breach_date'] = breach_date

                documents.append(record)
            else:
                skipped += 1
                skipped_lines.append(line)

            # Check if the number of documents reaches the chunk size
            if len(documents) == chunk_size:
                chunk_count += 1  # Increment chunk count
                print('Inserting chunk', chunk_count)
                collection.insert_many(documents)
                print('Inserted:', cnt)
                documents = []  # Reset the documents list

        # Insert the remaining documents (if any) after processing all lines
        if documents:
            chunk_count += 1  # Increment chunk count
            print('Inserting chunk', chunk_count)
            collection.insert_many(documents)
            print('Inserted:', cnt)

    print('Processed', file_path)
    print('Skipped', skipped)

    # Write skipped lines to a separate file
    if skipped_lines:
        output_folder = 'SkippedLines'
        if not os.path.exists(output_folder):
            os.makedirs(output_folder)

        file_name = os.path.basename(file_path)
        new_file_path = os.path.join(output_folder, f'unexpected_{file_name}')
        with open(new_file_path, 'w', encoding='utf-8') as saved_lines_file:
            saved_lines_file.write('\n'.join(skipped_lines))

# Function to process a single file based on its extension
def process_file(file_path, delimiter, pattern, chunk_size, breach_date, source_name):
    # Determine the file type based on the extension
    file_extension = os.path.splitext(file_path)[1].lower()

    # Check if the file size exceeds the maximum allowed
    if os.path.getsize(file_path) > max_file_size:
        split_and_process_large_file(file_path, delimiter, pattern, chunk_size, breach_date, source_name)
    else:
        if file_extension == '.csv':
            # Process CSV file
            process_csv(file_path, delimiter, pattern, chunk_size)
        elif file_extension == '.json':
            # Process JSON file
            process_json(file_path, pattern, chunk_size)
        elif file_extension == '.sql':
            # Process SQL file
            process_sql(file_path, pattern)
        elif file_extension == '.txt':
            # Process TXT file
            process_txt_file(file_path, delimiter, pattern, chunk_size, breach_date, source_name)
        else:
            print(f"Unsupported file type: {file_extension}")

# Function to split a large file into chunks and process each chunk
def split_and_process_large_file(file_path, delimiter, pattern, chunk_size, breach_date, source_name):
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
        chunk_count = 0
        documents = []

        for line in file:
            if delimiter in line:
                email, password = line.split(delimiter, 1)
                domain_name = email.split('@')[-1]

                record = base_obj.copy()
                record['email'] = email
                if len(password) >= 16:
                    record['password_hash'] = password
                else:
                    record['password'] = password
                record['source'] = source_name
                record['domainName'] = domain_name
                record['breach_date'] = breach_date

                documents.append(record)

            # Check if the number of documents reaches the chunk size
            if len(documents) == chunk_size:
                chunk_count += 1
                print(f'Inserting chunk {chunk_count} for large file')
                collection.insert_many(documents)
                documents = []

        # Insert the remaining documents (if any) after processing all lines
        if documents:
            chunk_count += 1
            print(f'Inserting chunk {chunk_count} for large file')
            collection.insert_many(documents)

# Prompt user for delimiter for text files
delimiter = input("Enter the delimiter for text files (default is ':'): ")
if not delimiter:
    delimiter = ":"

# Prompt user for folder or file path
path = input("Enter the file or folder path: ")

# Check if the path is a directory
if os.path.isdir(path):
    # Rename files in the folder
    file_list = os.listdir(path)
    file_list.sort()  # Sort the list of files

    for i, filename in enumerate(file_list):
        new_filename = f"{i}{os.path.splitext(filename)[1]}"  # Keep the file extension
        new_filepath = os.path.join(path, new_filename)
        old_filepath = os.path.join(path, filename)
        
        try:
            os.rename(old_filepath, new_filepath)
            print(f'Renamed "{filename}" to "{new_filename}"')
        except Exception as e:
            print(f'Error renaming "{filename}": {e}')

    # Get the current date
    current_date = datetime.now().strftime('%Y-%m-%d')

    # Prompt user for breach date
    breach_date = input(f"Enter the breach date (default is '{current_date}'): ") or current_date

    # Prompt user for source name
    source_name = input("Enter the source name: ")

    # Iterate over files in the directory
    for filename in os.listdir(path):
        file_path = os.path.join(path, filename)
        process_file(file_path, delimiter, source_name, chunk_size, breach_date, source_name)
else:
    # Get the current date
    current_date = datetime.now().strftime('%Y-%m-%d')

    # Prompt user for breach date
    breach_date = input(f"Enter the breach date (default is '{current_date}'): ") or current_date

    # Prompt user for source name
    source_name = input("Enter the source name: ")

    # Process a single file
    process_file(path, delimiter, source_name, chunk_size, breach_date, source_name)

# Disconnect from MongoDB
client.close()
print('Done!')
